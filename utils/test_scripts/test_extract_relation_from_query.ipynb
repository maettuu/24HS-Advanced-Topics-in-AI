{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set app as default directory to address imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), './../../'))\n",
    "\n",
    "#activate autoreload to easier test classes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata loaded successfully from JSON files.\n",
      "Initializing SPARQLGraph\n",
      "Graph loaded with 94107 triples after 0:00:02.678702\n"
     ]
    }
   ],
   "source": [
    "from app.services.sparql_graph import SPARQLGraph\n",
    "from app.config.enums import Environment\n",
    "graph = SPARQLGraph(Environment.DEV, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: Who is the director of Star Wars: Episode VI - Return of the Jedi?\n",
      "Most similar relation: director\n",
      "\n",
      "User Query: Can you tell me who directed the movie\n",
      "Most similar relation: director\n",
      "\n",
      "User Query: Who was the director of the movie X?\n",
      "Most similar relation: director\n",
      "\n",
      "User Query: Do you know who directed X?\n",
      "Most similar relation: director\n",
      "\n",
      "User Query: Who is the screenwriter of The Masked Gang: Cyprus?\n",
      "Most similar relation: screenwriter\n",
      "\n",
      "User Query: Can you tell me who wrote the script for The Masked Gang: Cyprus?\n",
      "Most similar relation: screenwriter\n",
      "\n",
      "User Query: Who was responsible for writing X?\n",
      "Most similar relation: author\n",
      "\n",
      "User Query: Who worked as the screenwriter on The Masked Gang: Cyprus?\n",
      "Most similar relation: screenwriter\n",
      "\n",
      "User Query: When was X released?\n",
      "Most similar relation: time period\n",
      "\n",
      "User Query: What is the release date of X?\n",
      "Most similar relation: publication date\n",
      "\n",
      "User Query: Can you tell me when X was first released?\n",
      "Most similar relation: time period\n",
      "\n",
      "User Query: When did X come out?\n",
      "Most similar relation: time period\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Define the relations from the graph\n",
    "relations = graph.get_relations_labels()\n",
    "\n",
    "# Array of paraphrased queries\n",
    "queries = [\n",
    "    # Original + Variations for \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\"\n",
    "    \"Who is the director of Star Wars: Episode VI - Return of the Jedi?\",\n",
    "    \"Can you tell me who directed the movie\",\n",
    "    \"Who was the director of the movie X?\",\n",
    "    \"Do you know who directed X?\",\n",
    "    \n",
    "    # Original + Variations for \"Who is the screenwriter of The Masked Gang: Cyprus?\"\n",
    "    \"Who is the screenwriter of The Masked Gang: Cyprus?\",\n",
    "    \"Can you tell me who wrote the script for The Masked Gang: Cyprus?\",\n",
    "    \"Who was responsible for writing X?\",\n",
    "    \"Who worked as the screenwriter on The Masked Gang: Cyprus?\",\n",
    "\n",
    "    # Original + Variations for \"When was 'The Godfather' released?\"\n",
    "    \"When was X released?\",\n",
    "    \"What is the release date of X?\",\n",
    "    \"Can you tell me when X was first released?\",\n",
    "    \"When did X come out?\"\n",
    "]\n",
    "\n",
    "# Convert relations into embeddings\n",
    "relation_embeddings = model.encode(relations, convert_to_tensor=True)\n",
    "\n",
    "# Loop through each query, compute similarity, and print the most similar relation\n",
    "for user_query in queries:\n",
    "    # Convert user query into embedding\n",
    "    query_embedding = model.encode(user_query, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, relation_embeddings)\n",
    "    \n",
    "    # Find the most similar relation\n",
    "    most_similar_idx = cosine_scores.argmax()\n",
    "    \n",
    "    # Print the most similar relation for the current query\n",
    "    print(f\"User Query: {user_query}\")\n",
    "    print(f\"Most similar relation: {relations[most_similar_idx]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4904)\n"
     ]
    }
   ],
   "source": [
    "query_embedding = model.encode(\"Test\", convert_to_tensor=True)\n",
    "    \n",
    "# Compute cosine similarities\n",
    "cosine_scores = util.pytorch_cos_sim(query_embedding, relation_embeddings)\n",
    "\n",
    "cosine_scores = cosine_scores.flatten()\n",
    "# Find the most similar relation\n",
    "most_similar_idx = cosine_scores.argmax()\n",
    "print(cosine_scores[most_similar_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Who is the director of MOVIE_NAME?\n",
      "Masked Query: director ?\n",
      "Most similar relation: director\n",
      "\n",
      "Original Query: Can you tell me who directed MOVIE_NAME?\n",
      "Masked Query: tell directed ?\n",
      "Most similar relation: director\n",
      "\n",
      "Original Query: Who was the director of the movie MOVIE_NAME?\n",
      "Masked Query: director movie ?\n",
      "Most similar relation: director\n",
      "\n",
      "Original Query: Do you know who directed MOVIE_NAME?\n",
      "Masked Query: know directed ?\n",
      "Most similar relation: director\n",
      "\n",
      "Original Query: Who is the screenwriter of MOVIE_NAME?\n",
      "Masked Query: screenwriter ?\n",
      "Most similar relation: screenwriter\n",
      "\n",
      "Original Query: Can you tell me who wrote the script for MOVIE_NAME?\n",
      "Masked Query: tell wrote script ?\n",
      "Most similar relation: screenwriter\n",
      "\n",
      "Original Query: Who was responsible for writing MOVIE_NAME?\n",
      "Masked Query: responsible writing ?\n",
      "Most similar relation: writing language\n",
      "\n",
      "Original Query: Who worked as the screenwriter on MOVIE_NAME?\n",
      "Masked Query: worked screenwriter ?\n",
      "Most similar relation: screenwriter\n",
      "\n",
      "Original Query: When was MOVIE_NAME released?\n",
      "Masked Query: released?\n",
      "Most similar relation: published in\n",
      "\n",
      "Original Query: What is the release date of MOVIE_NAME?\n",
      "Masked Query: release date ?\n",
      "Most similar relation: publication date\n",
      "\n",
      "Original Query: Can you tell me when MOVIE_NAME was first released?\n",
      "Masked Query: tell first released?\n",
      "Most similar relation: first appearance\n",
      "\n",
      "Original Query: When did the movie MOVIE_NAME come out?\n",
      "Masked Query: movie come out?\n",
      "Most similar relation: film crew member\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sample relations from the graph\n",
    "relations = graph.get_relations_labels()\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load stop words list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to mask the movie name and remove stop words\n",
    "def mask_movie_name(query):\n",
    "    # Step 1: Mask the movie name\n",
    "    # remove MOVIE_NAME from the query\n",
    "    masked_query = re.sub(r'MOVIE_NAME', '', query)\n",
    "    \n",
    "    # Step 2: Remove stop words\n",
    "    # Convert query to lowercase and split into words\n",
    "    query_words = masked_query.lower().split()\n",
    "    \n",
    "    # Filter out stop words\n",
    "    filtered_words = [word for word in query_words if word not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a cleaned query string\n",
    "    cleaned_query = \" \".join(filtered_words)\n",
    "    \n",
    "    return cleaned_query\n",
    "\n",
    "# Queries to process\n",
    "queries = [\n",
    "    # Queries related to the director of a movie\n",
    "    \"Who is the director of MOVIE_NAME?\",\n",
    "    \"Can you tell me who directed MOVIE_NAME?\",\n",
    "    \"Who was the director of the movie MOVIE_NAME?\",\n",
    "    \"Do you know who directed MOVIE_NAME?\",\n",
    "\n",
    "    # Queries related to the screenwriter of a movie\n",
    "    \"Who is the screenwriter of MOVIE_NAME?\",\n",
    "    \"Can you tell me who wrote the script for MOVIE_NAME?\",\n",
    "    \"Who was responsible for writing MOVIE_NAME?\",\n",
    "    \"Who worked as the screenwriter on MOVIE_NAME?\",\n",
    "\n",
    "\n",
    "    # Queries related to the release date of a movie\n",
    "    \"When was MOVIE_NAME released?\",\n",
    "    \"What is the release date of MOVIE_NAME?\",\n",
    "    \"Can you tell me when MOVIE_NAME was first released?\",\n",
    "    \"When did the movie MOVIE_NAME come out?\"\n",
    "]\n",
    "\n",
    "\n",
    "# Convert relations into embeddings\n",
    "relation_embeddings = model.encode(relations, convert_to_tensor=True)\n",
    "\n",
    "# Loop through each query, mask the movie name, compute similarity, and print the most similar relation\n",
    "for user_query in queries:\n",
    "    # Mask the movie name in the query\n",
    "    masked_query = mask_movie_name(user_query)\n",
    "    \n",
    "    # Convert masked query into embedding\n",
    "    query_embedding = model.encode(masked_query, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, relation_embeddings)\n",
    "    \n",
    "    # Find the most similar relation\n",
    "    most_similar_idx = cosine_scores.argmax()\n",
    "    \n",
    "    # Print the original and masked query, and the most similar relation\n",
    "    print(f\"Original Query: {user_query}\")\n",
    "    print(f\"Masked Query: {masked_query}\")\n",
    "    print(f\"Most similar relation: {relations[most_similar_idx]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Who worked as the screenwriter on 'The Godfather'?\n",
      "Cleaned Query: worked as screenwriter on ?\n",
      "Original Query: When did the movie MOVIE_NAME come out?\n",
      "Masked Query: movie come out?\n",
      "Most similar relation: screenwriter\n",
      "\n",
      "Original Query: When was 'The Godfather' released?\n",
      "Cleaned Query: released?\n",
      "Original Query: When did the movie MOVIE_NAME come out?\n",
      "Masked Query: movie come out?\n",
      "Most similar relation: published in\n",
      "\n",
      "Original Query: What is the release date of 'The Godfather'?\n",
      "Cleaned Query: what release date ?\n",
      "Original Query: When did the movie MOVIE_NAME come out?\n",
      "Masked Query: movie come out?\n",
      "Most similar relation: publication date\n",
      "\n",
      "Original Query: Can you tell me when 'The Godfather' was first released?\n",
      "Cleaned Query: you released?\n",
      "Original Query: When did the movie MOVIE_NAME come out?\n",
      "Masked Query: movie come out?\n",
      "Most similar relation: published in\n",
      "\n",
      "Original Query: When did the movie 'The Godfather' come out?\n",
      "Cleaned Query: did movie come out?\n",
      "Original Query: When did the movie MOVIE_NAME come out?\n",
      "Masked Query: movie come out?\n",
      "Most similar relation: original film format\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Custom stop words list that excludes domain-specific terms like 'release', 'date', 'come', etc.\n",
    "custom_stop_words = {\"who\", \"the\", \"of\", \"is\", \"was\", \"when\", \"can\", \"tell\", \"me\", \"first\"}\n",
    "\n",
    "# List of key phrases to preserve\n",
    "key_phrases = [\"release date\", \"come out\", \"published in\", \"first appearance\", \"screenwriter\", \"director\"]\n",
    "\n",
    "# Function to preprocess the query by masking movie names and preserving important context\n",
    "def mask_movie_name(query):\n",
    "    # Step 1: Mask the movie name\n",
    "    movie_name_pattern = r\"'(.*?)'|\\\"(.*?)\\\"\"  # Regex to find movie names enclosed in quotes\n",
    "    masked_query = re.sub(movie_name_pattern, \"\", query)\n",
    "    \n",
    "    # Step 2: Remove stop words but preserve key phrases\n",
    "    # Convert to lowercase and split into words\n",
    "    query_words = masked_query.lower().split()\n",
    "    \n",
    "    # Filter out custom stop words\n",
    "    filtered_words = [word for word in query_words if word not in custom_stop_words]\n",
    "    \n",
    "    # Rejoin filtered words\n",
    "    filtered_query = \" \".join(filtered_words)\n",
    "    \n",
    "    # Step 3: Ensure key phrases are preserved\n",
    "    for phrase in key_phrases:\n",
    "        if phrase in query.lower():\n",
    "            filtered_query = re.sub(phrase, phrase, filtered_query)\n",
    "    \n",
    "    return filtered_query\n",
    "\n",
    "# Example queries\n",
    "queries = [\n",
    "    \"Who worked as the screenwriter on 'The Godfather'?\",\n",
    "    \"When was 'The Godfather' released?\",\n",
    "    \"What is the release date of 'The Godfather'?\",\n",
    "    \"Can you tell me when 'The Godfather' was first released?\",\n",
    "    \"When did the movie 'The Godfather' come out?\"\n",
    "]\n",
    "\n",
    "# Apply the improved mask_movie_name function to each query\n",
    "for query in queries:\n",
    "    cleaned_query = mask_movie_name(query)\n",
    "    print(f\"Original Query: {query}\")\n",
    "    print(f\"Cleaned Query: {cleaned_query}\")\n",
    "    query_embedding = model.encode(cleaned_query, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, relation_embeddings)\n",
    "    \n",
    "    # Find the most similar relation\n",
    "    most_similar_idx = cosine_scores.argmax()\n",
    "    \n",
    "    # Print the original and masked query, and the most similar relation\n",
    "    print(f\"Original Query: {user_query}\")\n",
    "    print(f\"Masked Query: {masked_query}\")\n",
    "    print(f\"Most similar relation: {relations[most_similar_idx]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original User Query: Who directed\n",
      "Cleaned Query: directed\n",
      "Most similar relation: director\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# List of additional common phrases to remove\n",
    "additional_phrases = [\n",
    "    \"who\", \"tell me\", \"can you tell me\", \"could you\", \"i think\", \"was\", \"is\", \"the\", \"of\", \"and\", \"when\", \"what\", \"movie\"\n",
    "    # if we remove movie we get better stuff for director which is weird\n",
    "]\n",
    "\n",
    "# Function to clean and preprocess the query\n",
    "def preprocess_query(query):\n",
    "    # Convert to lowercase\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Step 1: Remove additional phrases\n",
    "    for phrase in additional_phrases:\n",
    "        query = re.sub(r'\\b' + re.escape(phrase) + r'\\b', '', query)\n",
    "    \n",
    "    # Step 2: Remove stop words\n",
    "    query_words = query.split()\n",
    "    filtered_words = [word for word in query_words if word not in stop_words]\n",
    "    \n",
    "    # Rejoin the filtered words into a cleaned query string\n",
    "    cleaned_query = \" \".join(filtered_words).strip()\n",
    "    \n",
    "    return cleaned_query\n",
    "\n",
    "# Example query\n",
    "# user_query = \"Can you tell me who wrote the screenplay for 'The Godfather'?\"\n",
    "# user_query = \"When was the movie released?\"\n",
    "user_query = \"Who directed\"\n",
    "\n",
    "# Step 3: Preprocess the query\n",
    "cleaned_query = preprocess_query(user_query)\n",
    "\n",
    "# Load the transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Sample relations from the graph (you would replace this with your actual relations)\n",
    "relations = graph.get_relations_labels()\n",
    "\n",
    "# Step 4: Compute relation embeddings\n",
    "relation_embeddings = model.encode(relations, convert_to_tensor=True)\n",
    "\n",
    "# Step 5: Encode the cleaned query and compute similarity\n",
    "query_embedding = model.encode(cleaned_query, convert_to_tensor=True)\n",
    "cosine_scores = util.pytorch_cos_sim(query_embedding, relation_embeddings)\n",
    "\n",
    "# Step 6: Find the most similar relation\n",
    "most_similar_idx = cosine_scores.argmax()\n",
    "\n",
    "# Output the results\n",
    "print(f\"Original User Query: {user_query}\")\n",
    "print(f\"Cleaned Query: {cleaned_query}\")\n",
    "print(f\"Most similar relation: {relations[most_similar_idx]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'publish', 'unfreeze', 'issue', 'discharge', 'unloose', 'liberate', 'bring_out', 'let_go_of', 'relinquish', 'secrete', 'resign', 'give_up', 'unloosen', 'loose', 'free', 'expel', 'exhaust', 'turn', 'eject', 'unblock', 'let_go', 'put_out', 'release'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Example usage\n",
    "synonyms = get_synonyms(\"released\")\n",
    "print(synonyms)  # Could include terms like \"publication\", \"launch\", etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\adam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams extracted: ['tell release', 'release date', 'date Inception']\n",
      "Cosine similarity: tensor([ 0.1188,  0.0998,  0.1987,  0.2629,  0.1713,  0.0906,  0.1797,  0.2561,\n",
      "         0.1208,  0.2855,  0.2235,  0.2089,  0.1138,  0.2049,  0.2717,  0.1884,\n",
      "         0.1105,  0.2477,  0.1864, -0.0122,  0.1661,  0.1852,  0.2064,  0.1725,\n",
      "         0.2165,  0.0435,  0.1647,  0.1851,  0.3142,  0.2061,  0.0998,  0.2491,\n",
      "         0.0841,  0.1790,  0.1812,  0.1259,  0.1812,  0.3307,  0.0884,  0.1524,\n",
      "         0.1666,  0.1644,  0.1304,  0.1589,  0.1670,  0.3560,  0.1180,  0.1336,\n",
      "         0.0926,  0.1659,  0.2088,  0.0993,  0.2596,  0.1823,  0.1926,  0.1732,\n",
      "         0.2436,  0.0491,  0.0803,  0.1198,  0.1902,  0.1331,  0.1898,  0.1683,\n",
      "         0.2164,  0.1689,  0.0939,  0.1859,  0.0760,  0.0631,  0.3519,  0.2288,\n",
      "         0.2405,  0.1886,  0.2791,  0.1844,  0.0902,  0.1957,  0.1254,  0.0765,\n",
      "         0.1765,  0.1650,  0.2236,  0.0777,  0.0440,  0.0688,  0.1629,  0.1829,\n",
      "         0.1670,  0.2199,  0.2215,  0.1053,  0.1580,  0.1741,  0.1360,  0.1634,\n",
      "         0.1242,  0.1791,  0.1058,  0.1904,  0.1125,  0.1758,  0.2874,  0.1688,\n",
      "         0.0466,  0.1704,  0.1977,  0.1292,  0.1800,  0.1713,  0.1548,  0.3927,\n",
      "         0.1646,  0.0629,  0.1966,  0.1200,  0.1960,  0.1194,  0.1916,  0.1309,\n",
      "         0.3179,  0.0668,  0.1141,  0.0951,  0.1765,  0.2457,  0.2120,  0.1684,\n",
      "         0.1360,  0.2472,  0.1819,  0.2964,  0.2021,  0.1406,  0.0985,  0.2654,\n",
      "         0.2294,  0.2056,  0.1450,  0.2067,  0.1016,  0.1610,  0.1179,  0.1624,\n",
      "         0.1869,  0.1675,  0.1198,  0.2381,  0.2038,  0.1381,  0.1795,  0.2214,\n",
      "         0.0998,  0.1239,  0.1883,  0.1842,  0.2445,  0.1462,  0.1780,  0.1566,\n",
      "         0.2909,  0.1503,  0.1987,  0.3192,  0.2222,  0.1852,  0.0348,  0.1128,\n",
      "         0.1799,  0.1866,  0.1379,  0.1572,  0.0938,  0.2047,  0.1865,  0.2059,\n",
      "         0.1647,  0.1954,  0.1919,  0.2022,  0.1858,  0.1155,  0.3254,  0.1511,\n",
      "         0.0491,  0.1538,  0.1212,  0.1797,  0.1924,  0.1223,  0.1625,  0.2178,\n",
      "         0.0870,  0.1884,  0.1524,  0.0997,  0.0735,  0.1193,  0.1778,  0.1165,\n",
      "         0.2853,  0.2159,  0.2032,  0.1671,  0.1954, -0.0284,  0.2586,  0.1617,\n",
      "         0.2297,  0.1112,  0.1704,  0.2441,  0.1387,  0.3824,  0.0696,  0.1645,\n",
      "         0.2274,  0.1689,  0.2209,  0.1162,  0.1815,  0.2151,  0.2056,  0.4143,\n",
      "         0.1578,  0.1486,  0.2779,  0.1834,  0.1774,  0.2751,  0.1264,  0.4898,\n",
      "         0.1291,  0.0746,  0.1691,  0.0415,  0.1718,  0.2406,  0.2956,  0.1452,\n",
      "         0.1297,  0.1461,  0.0771,  0.1115,  0.1005,  0.1790,  0.1574,  0.1301,\n",
      "         0.2233,  0.1969,  0.1387,  0.2119,  0.2095,  0.1668,  0.5885])\n",
      "Most similar relation: publication date\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "relations_with_synonyms = {\n",
    "    \"release date\": [\"release date\", \"launch date\", \"publication date\", \"when was it released\"],\n",
    "    \"director\": [\"director\", \"film director\", \"who directed\"],\n",
    "    # Add more relations and synonyms\n",
    "}\n",
    "\n",
    "# Ensure you have the required data for bigrams\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def extract_bigrams(text):\n",
    "    # Tokenize the sentence\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "    # Generate bigrams\n",
    "    bigrams = list(nltk.bigrams(filtered_tokens))\n",
    "    # Join bigrams into phrases\n",
    "    bigram_phrases = [' '.join(bigram) for bigram in bigrams]\n",
    "    return bigram_phrases\n",
    "\n",
    "# Example query\n",
    "query = \"\"\n",
    "\n",
    "# Extract bigrams\n",
    "bigrams = extract_bigrams(query)\n",
    "print(\"Bigrams extracted:\", bigrams)\n",
    "\n",
    "# Embed each bigram and find the most similar relation\n",
    "bigram_embeddings = model.encode(bigrams, convert_to_tensor=True)\n",
    "\n",
    "# Find the most similar relation\n",
    "cosine_scores = util.pytorch_cos_sim(bigram_embeddings, relation_embeddings)\n",
    "max_scores, _ = cosine_scores.max(dim=0)\n",
    "\n",
    "# Get the index of the most similar relation\n",
    "most_similar_idx = max_scores.argmax()\n",
    "\n",
    "# Get the most similar relation based on the correct index\n",
    "most_similar_relation = relations[most_similar_idx]\n",
    "\n",
    "print(\"Cosine similarity:\", \n",
    "print(\"Most similar relation:\", most_similar_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_relation(self, query):\n",
    "    \"\"\"\n",
    "    Finds the most similar relation to the given query based on cosine similarity for each bigram.\n",
    "    :param query: The user query as a string.\n",
    "    :return: The most similar relation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract bigrams from the query\n",
    "    bigrams = extract_bigrams(query)  # Assuming you have the extract_bigrams function implemented\n",
    "    print(f\"Bigrams extracted: {bigrams}\")\n",
    "    \n",
    "    # Step 2: Encode each bigram\n",
    "    bigram_embeddings = self.transformer_model.encode(bigrams, convert_to_tensor=True)\n",
    "    \n",
    "    # Step 3: Compute cosine similarities between each bigram and all relation embeddings\n",
    "    cosine_scores = util.pytorch_cos_sim(bigram_embeddings, self.relation_embeddings)\n",
    "    \n",
    "    # Step 4: Iterate over each bigram and print cosine similarity scores\n",
    "    for i, bigram in enumerate(bigrams):\n",
    "        print(f\"\\nCosine similarities for bigram: '{bigram}'\")\n",
    "        for j, relation in enumerate(self.relations):\n",
    "            similarity_score = cosine_scores[i, j].item()  # Get the similarity score for the bigram and relation\n",
    "            print(f\"  Similarity with relation '{relation}': {similarity_score:.4f}\")\n",
    "    \n",
    "    # Step 5: Reduce the scores to find the most similar relation (e.g., by max similarity)\n",
    "    max_scores, _ = cosine_scores.max(dim=0)  # Maximize over bigrams for each relation\n",
    "    most_similar_idx = max_scores.argmax()  # Get the index of the most similar relation\n",
    "\n",
    "    # Step 6: Get and print the most similar relation and its cosine similarity\n",
    "    most_similar_relation = self.relations[most_similar_idx]\n",
    "    most_similar_distance = max_scores[most_similar_idx].item()\n",
    "    print(f\"\\nMost similar relation: '{most_similar_relation}' with cosine similarity: {most_similar_distance:.4f}\")\n",
    "    \n",
    "    # Step 7: Return the most similar relation if it passes the threshold\n",
    "    if most_similar_distance < self.threshold:\n",
    "        return None\n",
    "    return most_similar_relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atai-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
