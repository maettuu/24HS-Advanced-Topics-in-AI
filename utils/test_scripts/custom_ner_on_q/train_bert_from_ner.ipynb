{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:01:13.714282Z",
     "start_time": "2024-10-19T11:01:11.963764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipywidgets) (8.28.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\adam\\anaconda3\\envs\\atai-bot\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# %pip install transformers[torch] datasets accelerate>=0.26.0\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:01:13.795092Z",
     "start_time": "2024-10-19T11:01:13.791140Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:01:13.820516Z",
     "start_time": "2024-10-19T11:01:13.812342Z"
    }
   },
   "outputs": [],
   "source": [
    "# import labels and relationships from the json files\n",
    "import json\n",
    "with open('./../../useful_dataset/graph/unique_movies.json') as f:\n",
    "    file = json.load(f)\n",
    "    entities = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:01:13.933253Z",
     "start_time": "2024-10-19T11:01:13.928728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25523"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:01:13.977280Z",
     "start_time": "2024-10-19T11:01:13.973585Z"
    }
   },
   "outputs": [],
   "source": [
    "templates = [\n",
    "    # Director-related queries\n",
    "    \"Who directed {}\",\n",
    "    \"Can you tell me who the director of {} is\",\n",
    "    \"Who is behind the camera for '{}'\",\n",
    "\n",
    "    # Screenwriter-related queries\n",
    "    \"Who wrote the screenplay for {}\",\n",
    "    \"Which writer worked on {}\",\n",
    "    \"Who is credited as the screenwriter of {}\",\n",
    "\n",
    "    # Actor-related queries\n",
    "    \"Which actors starred in {}\",\n",
    "    \"Who are the main actors in '{}'\",\n",
    "    \"Who played in {}\",\n",
    "    \"Can you name any actors from {}\",\n",
    "\n",
    "    # Recommendations based on movies\n",
    "    \"If I enjoyed {}, {}, {}, and {} what other movies might I like\",\n",
    "    \"If I enjoyed {}, {}, {}, {}, and {} what other movies might I like\",\n",
    "    \"What are some movies similar to {}\",\n",
    "    \"Can you recommend films like {}, {}, {}, and {}\",\n",
    "    \"Can you recommend films like {}, {}, {}, {}, and {}\",\n",
    "    \"I loved {}, {}, {}, and {}. Any recommendations for similar movies\",\n",
    "    \"I loved {}, {}, {}, {}, and {}. Any recommendations for similar movies\",\n",
    "    \"What movies are in the same genre as {}\",\n",
    "\n",
    "    # Plot-related queries\n",
    "    \"What is {} about\",\n",
    "    \"Can you summarize the plot of {}\",\n",
    "    \"What happens in {}\",\n",
    "\n",
    "    # Miscellaneous\n",
    "    \"What genre is \\\"{}\\\"\",\n",
    "    \"Who composed the music for {}\",\n",
    "    \"When was {} released\",\n",
    "    \"What awards has \\\"{}\\\" won\",\n",
    "    \"Is {} part of a series or franchise\",\n",
    "    \"Are there any sequels or prequels to {}\",\n",
    "    \"Where can I watch {}\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:01:14.051406Z",
     "start_time": "2024-10-19T11:01:13.988955Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_examples(movies, templates, num_examples=100):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_examples):\n",
    "        # Randomly pick a relationship and a movie title\n",
    "        # Randomly pick a template\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        # Determine the number of placeholders in the template\n",
    "        num_placeholders = template.count('{}')\n",
    "\n",
    "        # Randomly pick the required number of movies\n",
    "        selected_movies = random.sample(movies, num_placeholders)\n",
    "\n",
    "        # remove \", -, ., : ! ? ;  from the movie\" in a simple way\n",
    "        # selected_movies = [movie.translate(str.maketrans('', '', ',-.:!?;')) for movie in selected_movies]\n",
    "\n",
    "\n",
    "        \n",
    "        # Format the template with the selected movies\n",
    "        question = template.format(*selected_movies)\n",
    "\n",
    "        # Tokenize question manually\n",
    "        tokens = question.split()\n",
    "\n",
    "        # Create labels for each token\n",
    "        label_seq = []\n",
    "        for token in tokens:\n",
    "            labeled = False\n",
    "            for movie in selected_movies:\n",
    "                if token == movie.split(\" \")[0]:\n",
    "                    label_seq.append(\"B-MOVIE\")\n",
    "                    labeled = True\n",
    "                    break\n",
    "                elif token in movie.split()[1:]:\n",
    "                    label_seq.append(\"I-MOVIE\")\n",
    "                    labeled = True\n",
    "                    break\n",
    "            if not labeled:\n",
    "                label_seq.append(\"O\")\n",
    "\n",
    "        # tokens to lower\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "\n",
    "        # Append to the list\n",
    "        sentences.append(tokens)\n",
    "        labels.append(label_seq)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Generate 200 examples\n",
    "tokens, ner_tags = generate_examples(entities, templates, num_examples=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:01:14.070339Z",
     "start_time": "2024-10-19T11:01:14.066120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can', 'you', 'name', 'any', 'actors', 'from', 'naruto', 'shippuden', 'the', 'movie:', 'the', 'lost', 'tower']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-MOVIE', 'I-MOVIE', 'I-MOVIE', 'I-MOVIE', 'I-MOVIE', 'I-MOVIE', 'I-MOVIE']\n"
     ]
    }
   ],
   "source": [
    "id = 132\n",
    "print(tokens[id])\n",
    "print(ner_tags[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:01:14.096664Z",
     "start_time": "2024-10-19T11:01:14.085794Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a checkpoint of the created dataset with pickle?\n",
    "import pickle\n",
    "\n",
    "with open('data_for_ner.pkl', 'wb') as f:\n",
    "    pickle.dump((tokens, ner_tags), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:01:15.975084Z",
     "start_time": "2024-10-19T11:01:14.110631Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06a8db92efa4b5aafb4d7a45b6f954f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835c8813d76d405db1c0b47f330cd62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "import random\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Load your dataset in the correct format\n",
    "train_data = {\n",
    "    'tokens': tokens,\n",
    "    'ner_tags': ner_tags\n",
    "}\n",
    "\n",
    "# Map NER tags to labels (B-RELATION, B-MOVIE, etc.)\n",
    "label_list = [\"O\", \"B-MOVIE\", \"I-MOVIE\"]\n",
    "\n",
    "# Create the mapping from label to index\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# Convert ner_tags to numerical values based on label_map\n",
    "def convert_labels_to_ids(ner_tags):\n",
    "    converted_tags = []\n",
    "    for labels in ner_tags:\n",
    "        converted_tags.append([label_map[label] for label in labels])\n",
    "    return converted_tags\n",
    "\n",
    "# Apply label conversion\n",
    "train_data['ner_tags'] = convert_labels_to_ids(train_data['ner_tags'])\n",
    "\n",
    "# Convert the dataset into Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict(train_data)\n",
    "\n",
    "# Split the dataset into training and validation sets (80/20 split)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "datasets = DatasetDict({\"train\": train_test_split[\"train\"], \"test\": train_test_split[\"test\"]})\n",
    "\n",
    "# Load a pre-trained NER-finetuned tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    ignore_mismatched_sizes=True  # Ignore mismatched sizes and initialize the classifier head for your label set\n",
    ")\n",
    "\n",
    "# Tokenize the dataset\n",
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'], \n",
    "        truncation=True,  # Activate truncation\n",
    "        # padding=True,     # Activate padding\n",
    "        is_split_into_words=True  # Ensure input is treated as pre-tokenized (word level)\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get word IDs for each token\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignored label for special tokens and padding\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])  # Assign the label to the first token of a word\n",
    "            else:\n",
    "                label_ids.append(-100)  # Assign ignored label to sub-tokens\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization and label alignment to the dataset\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:41:10.872216Z",
     "start_time": "2024-10-19T11:01:15.988941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adam\\anaconda3\\envs\\atai-bot\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382a26ea82ae4876a2b085054f65e9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8788, 'grad_norm': 2.002763509750366, 'learning_rate': 1.9733333333333336e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7064, 'grad_norm': 1.829643964767456, 'learning_rate': 1.9466666666666668e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5752, 'grad_norm': 1.1827261447906494, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4684, 'grad_norm': 1.119017481803894, 'learning_rate': 1.8933333333333334e-05, 'epoch': 0.27}\n",
      "{'loss': 0.4225, 'grad_norm': 1.4572210311889648, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.33}\n",
      "{'loss': 0.2958, 'grad_norm': 1.1690351963043213, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.4}\n",
      "{'loss': 0.203, 'grad_norm': 2.503082752227783, 'learning_rate': 1.8133333333333335e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1727, 'grad_norm': 3.6340677738189697, 'learning_rate': 1.7866666666666666e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1042, 'grad_norm': 2.7655110359191895, 'learning_rate': 1.76e-05, 'epoch': 0.6}\n",
      "{'loss': 0.1055, 'grad_norm': 1.9958871603012085, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.67}\n",
      "{'loss': 0.1166, 'grad_norm': 1.7925909757614136, 'learning_rate': 1.706666666666667e-05, 'epoch': 0.73}\n",
      "{'loss': 0.104, 'grad_norm': 1.105009913444519, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0815, 'grad_norm': 1.414961338043213, 'learning_rate': 1.6533333333333333e-05, 'epoch': 0.87}\n",
      "{'loss': 0.1113, 'grad_norm': 1.5146889686584473, 'learning_rate': 1.6266666666666668e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0808, 'grad_norm': 1.5005029439926147, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f4ac7607d348c4821f156d4eb847d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       MOVIE       0.93      0.89      0.91       990\n",
      "\n",
      "   micro avg       0.93      0.89      0.91       990\n",
      "   macro avg       0.93      0.89      0.91       990\n",
      "weighted avg       0.93      0.89      0.91       990\n",
      "\n",
      "{'eval_loss': 0.0769214853644371, 'eval_precision': 0.933972310969116, 'eval_recall': 0.8858585858585859, 'eval_f1': 0.9092794193882842, 'eval_runtime': 30.6911, 'eval_samples_per_second': 19.55, 'eval_steps_per_second': 1.238, 'epoch': 1.0}\n",
      "{'loss': 0.0847, 'grad_norm': 1.6641005277633667, 'learning_rate': 1.5733333333333334e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0849, 'grad_norm': 2.902892827987671, 'learning_rate': 1.546666666666667e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0917, 'grad_norm': 2.683138370513916, 'learning_rate': 1.5200000000000002e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0649, 'grad_norm': 1.7959421873092651, 'learning_rate': 1.4933333333333335e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0872, 'grad_norm': 1.6093648672103882, 'learning_rate': 1.4666666666666666e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0683, 'grad_norm': 1.6620006561279297, 'learning_rate': 1.4400000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0694, 'grad_norm': 2.4469103813171387, 'learning_rate': 1.4133333333333334e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0671, 'grad_norm': 0.25274011492729187, 'learning_rate': 1.3866666666666669e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0557, 'grad_norm': 0.6689409017562866, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0737, 'grad_norm': 3.0089027881622314, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0487, 'grad_norm': 0.912946343421936, 'learning_rate': 1.3066666666666668e-05, 'epoch': 1.73}\n",
      "{'loss': 0.0525, 'grad_norm': 0.7094433903694153, 'learning_rate': 1.2800000000000001e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0829, 'grad_norm': 1.9220433235168457, 'learning_rate': 1.2533333333333336e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0606, 'grad_norm': 1.6907901763916016, 'learning_rate': 1.2266666666666667e-05, 'epoch': 1.93}\n",
      "{'loss': 0.0519, 'grad_norm': 1.4911885261535645, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a55dbd84cbd41d384176670600642d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       MOVIE       0.95      0.89      0.92       990\n",
      "\n",
      "   micro avg       0.95      0.89      0.92       990\n",
      "   macro avg       0.95      0.89      0.92       990\n",
      "weighted avg       0.95      0.89      0.92       990\n",
      "\n",
      "{'eval_loss': 0.06668711453676224, 'eval_precision': 0.9463519313304721, 'eval_recall': 0.8909090909090909, 'eval_f1': 0.9177939646201873, 'eval_runtime': 25.7396, 'eval_samples_per_second': 23.31, 'eval_steps_per_second': 1.476, 'epoch': 2.0}\n",
      "{'loss': 0.0419, 'grad_norm': 0.6722897887229919, 'learning_rate': 1.1733333333333335e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0387, 'grad_norm': 1.158530831336975, 'learning_rate': 1.1466666666666668e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0279, 'grad_norm': 0.5009326338768005, 'learning_rate': 1.1200000000000001e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0468, 'grad_norm': 5.206861972808838, 'learning_rate': 1.0933333333333334e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0711, 'grad_norm': 3.227083921432495, 'learning_rate': 1.0666666666666667e-05, 'epoch': 2.33}\n",
      "{'loss': 0.062, 'grad_norm': 1.8128465414047241, 'learning_rate': 1.04e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0523, 'grad_norm': 0.6481795907020569, 'learning_rate': 1.0133333333333335e-05, 'epoch': 2.47}\n",
      "{'loss': 0.043, 'grad_norm': 0.3147745132446289, 'learning_rate': 9.866666666666668e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0347, 'grad_norm': 1.805090308189392, 'learning_rate': 9.600000000000001e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0457, 'grad_norm': 2.218958616256714, 'learning_rate': 9.333333333333334e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0457, 'grad_norm': 1.0482290983200073, 'learning_rate': 9.066666666666667e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0552, 'grad_norm': 0.9054969549179077, 'learning_rate': 8.8e-06, 'epoch': 2.8}\n",
      "{'loss': 0.0384, 'grad_norm': 0.8431414365768433, 'learning_rate': 8.533333333333335e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0546, 'grad_norm': 0.2811757028102875, 'learning_rate': 8.266666666666667e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0451, 'grad_norm': 1.028432011604309, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1dff07cfd24ce5a5b1482ded3d4a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       MOVIE       0.94      0.91      0.93       990\n",
      "\n",
      "   micro avg       0.94      0.91      0.93       990\n",
      "   macro avg       0.94      0.91      0.93       990\n",
      "weighted avg       0.94      0.91      0.93       990\n",
      "\n",
      "{'eval_loss': 0.048049092292785645, 'eval_precision': 0.9433962264150944, 'eval_recall': 0.9090909090909091, 'eval_f1': 0.9259259259259259, 'eval_runtime': 19.2555, 'eval_samples_per_second': 31.16, 'eval_steps_per_second': 1.973, 'epoch': 3.0}\n",
      "{'loss': 0.0446, 'grad_norm': 1.9249995946884155, 'learning_rate': 7.733333333333334e-06, 'epoch': 3.07}\n",
      "{'loss': 0.045, 'grad_norm': 2.155670404434204, 'learning_rate': 7.4666666666666675e-06, 'epoch': 3.13}\n",
      "{'loss': 0.0317, 'grad_norm': 2.195035457611084, 'learning_rate': 7.2000000000000005e-06, 'epoch': 3.2}\n",
      "{'loss': 0.0272, 'grad_norm': 0.9700429439544678, 'learning_rate': 6.9333333333333344e-06, 'epoch': 3.27}\n",
      "{'loss': 0.0342, 'grad_norm': 4.487111568450928, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.33}\n",
      "{'loss': 0.0279, 'grad_norm': 0.42687809467315674, 'learning_rate': 6.4000000000000006e-06, 'epoch': 3.4}\n",
      "{'loss': 0.0456, 'grad_norm': 0.3291325569152832, 'learning_rate': 6.133333333333334e-06, 'epoch': 3.47}\n",
      "{'loss': 0.0337, 'grad_norm': 2.084853410720825, 'learning_rate': 5.8666666666666675e-06, 'epoch': 3.53}\n",
      "{'loss': 0.0245, 'grad_norm': 0.7836925983428955, 'learning_rate': 5.600000000000001e-06, 'epoch': 3.6}\n",
      "{'loss': 0.0459, 'grad_norm': 0.24313580989837646, 'learning_rate': 5.333333333333334e-06, 'epoch': 3.67}\n",
      "{'loss': 0.0268, 'grad_norm': 0.9607810378074646, 'learning_rate': 5.0666666666666676e-06, 'epoch': 3.73}\n",
      "{'loss': 0.0634, 'grad_norm': 3.5394959449768066, 'learning_rate': 4.800000000000001e-06, 'epoch': 3.8}\n",
      "{'loss': 0.0211, 'grad_norm': 0.27798306941986084, 'learning_rate': 4.533333333333334e-06, 'epoch': 3.87}\n",
      "{'loss': 0.0289, 'grad_norm': 0.9280337691307068, 'learning_rate': 4.266666666666668e-06, 'epoch': 3.93}\n",
      "{'loss': 0.0189, 'grad_norm': 0.7546005845069885, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4cc795866445d49270ed377a020570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       MOVIE       0.94      0.92      0.93       990\n",
      "\n",
      "   micro avg       0.94      0.92      0.93       990\n",
      "   macro avg       0.94      0.92      0.93       990\n",
      "weighted avg       0.94      0.92      0.93       990\n",
      "\n",
      "{'eval_loss': 0.046344298869371414, 'eval_precision': 0.9392378990731205, 'eval_recall': 0.9212121212121213, 'eval_f1': 0.9301376848546661, 'eval_runtime': 14.8093, 'eval_samples_per_second': 40.515, 'eval_steps_per_second': 2.566, 'epoch': 4.0}\n",
      "{'loss': 0.0361, 'grad_norm': 0.6429855823516846, 'learning_rate': 3.7333333333333337e-06, 'epoch': 4.07}\n",
      "{'loss': 0.0229, 'grad_norm': 1.1059170961380005, 'learning_rate': 3.4666666666666672e-06, 'epoch': 4.13}\n",
      "{'loss': 0.0172, 'grad_norm': 0.8281331062316895, 'learning_rate': 3.2000000000000003e-06, 'epoch': 4.2}\n",
      "{'loss': 0.0376, 'grad_norm': 0.08562179654836655, 'learning_rate': 2.9333333333333338e-06, 'epoch': 4.27}\n",
      "{'loss': 0.0438, 'grad_norm': 2.7291312217712402, 'learning_rate': 2.666666666666667e-06, 'epoch': 4.33}\n",
      "{'loss': 0.0268, 'grad_norm': 0.6954255104064941, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.4}\n",
      "{'loss': 0.0203, 'grad_norm': 1.5691213607788086, 'learning_rate': 2.133333333333334e-06, 'epoch': 4.47}\n",
      "{'loss': 0.0229, 'grad_norm': 2.710425853729248, 'learning_rate': 1.8666666666666669e-06, 'epoch': 4.53}\n",
      "{'loss': 0.018, 'grad_norm': 0.2747434377670288, 'learning_rate': 1.6000000000000001e-06, 'epoch': 4.6}\n",
      "{'loss': 0.0236, 'grad_norm': 0.2166859358549118, 'learning_rate': 1.3333333333333334e-06, 'epoch': 4.67}\n",
      "{'loss': 0.0198, 'grad_norm': 1.2329851388931274, 'learning_rate': 1.066666666666667e-06, 'epoch': 4.73}\n",
      "{'loss': 0.0278, 'grad_norm': 1.710275411605835, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.8}\n",
      "{'loss': 0.0252, 'grad_norm': 0.39930692315101624, 'learning_rate': 5.333333333333335e-07, 'epoch': 4.87}\n",
      "{'loss': 0.0384, 'grad_norm': 2.8806982040405273, 'learning_rate': 2.666666666666667e-07, 'epoch': 4.93}\n",
      "{'loss': 0.0437, 'grad_norm': 1.7660934925079346, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df5b182723e4e0d9719c11654854be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       MOVIE       0.95      0.93      0.94       990\n",
      "\n",
      "   micro avg       0.95      0.93      0.94       990\n",
      "   macro avg       0.95      0.93      0.94       990\n",
      "weighted avg       0.95      0.93      0.94       990\n",
      "\n",
      "{'eval_loss': 0.04551567882299423, 'eval_precision': 0.9463364293085655, 'eval_recall': 0.9262626262626262, 'eval_f1': 0.9361919346605411, 'eval_runtime': 17.5028, 'eval_samples_per_second': 34.28, 'eval_steps_per_second': 2.171, 'epoch': 5.0}\n",
      "{'train_runtime': 1489.2611, 'train_samples_per_second': 8.058, 'train_steps_per_second': 0.504, 'train_loss': 0.09490114943186442, 'epoch': 5.0}\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Save the model and tokenizer\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./custom_ner_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./custom_ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adam\\anaconda3\\envs\\atai-bot\\Lib\\site-packages\\transformers\\modeling_utils.py:2830\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   2825\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m   2827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[0;32m   2828\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[0;32m   2829\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[1;32m-> 2830\u001b[0m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2832\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[1;32mc:\\Users\\adam\\anaconda3\\envs\\atai-bot\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[0;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Map label IDs back to label names\n",
    "id_to_label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Function to align predictions and compute metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id_to_label[label_id] for label_id in label_seq if label_id != -100]  # Ignore padding tokens\n",
    "        for label_seq in labels\n",
    "    ]\n",
    "    pred_labels = [\n",
    "        [id_to_label[pred_id] for pred_id, label_id in zip(pred_seq, label_seq) if label_id != -100]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Use seqeval to calculate metrics\n",
    "    precision = precision_score(true_labels, pred_labels)\n",
    "    recall = recall_score(true_labels, pred_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels)\n",
    "    \n",
    "    print(\"\\nClassification Report:\\n\", classification_report(true_labels, pred_labels))\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Define training arguments\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  # Save model checkpoints for each epoch\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    metric_for_best_model=\"f1\",  # Select the best model based on F1-score\n",
    "    load_best_model_at_end=True,  # Automatically load the best model\n",
    ")\n",
    "\n",
    "# Create Trainer instance with custom compute_metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./custom_ner_model\")\n",
    "tokenizer.save_pretrained(\"./custom_ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:41:11.106220Z",
     "start_time": "2024-10-19T11:41:10.895413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./custom_ner_model1\\\\tokenizer_config.json',\n",
       " './custom_ner_model1\\\\special_tokens_map.json',\n",
       " './custom_ner_model1\\\\vocab.txt',\n",
       " './custom_ner_model1\\\\added_tokens.json',\n",
       " './custom_ner_model1\\\\tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./custom_ner_model1\")\n",
    "tokenizer.save_pretrained(\"./custom_ner_model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities(input_text, model, tokenizer):\n",
    "    # Define the label list (same as the training script)\n",
    "    label_list = [\"O\", \"B-MOVIE\", \"I-MOVIE\"]\n",
    "    \n",
    "    # Split the input text into words\n",
    "    words = input_text.split()\n",
    "    \n",
    "    # Tokenize the input with `is_split_into_words=True`\n",
    "    tokenized_input = tokenizer(words, return_tensors=\"pt\", is_split_into_words=True, truncation=True)\n",
    "    \n",
    "    # Move inputs to GPU if available\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    inputs = {key: value.to(device) for key, value in tokenized_input.items()}\n",
    "\n",
    "    # Run the model and get predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    # Get the word IDs to map tokens back to words\n",
    "    word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "    \n",
    "    # Initialize variables\n",
    "    previous_word_idx = None\n",
    "    labels = []\n",
    "    \n",
    "    for word_idx, pred_id in zip(word_ids, predictions[0]):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        elif word_idx != previous_word_idx:\n",
    "            # Start of a new word\n",
    "            label = label_list[pred_id.item()]\n",
    "            labels.append(label)\n",
    "            previous_word_idx = word_idx\n",
    "    \n",
    "    # Combine words and their predicted labels\n",
    "    output_text = ''\n",
    "    for word, label in zip(words, labels):\n",
    "        if label != 'O':\n",
    "            output_text += f\"[{word} ({label})] \"\n",
    "        else:\n",
    "            output_text += f\"{word} \"\n",
    "    \n",
    "    print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:41:11.148715Z",
     "start_time": "2024-10-19T11:41:11.117995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the director of [star (B-MOVIE)] [wars (I-MOVIE)] [jedi (I-MOVIE)] [- (I-MOVIE)] [4? (I-MOVIE)] \n",
      "Given that I like [The (B-MOVIE)] [Lion (I-MOVIE)] King, Pocahontas, [and (I-MOVIE)] [The (B-MOVIE)] [Beauty (I-MOVIE)] [and (I-MOVIE)] [the (I-MOVIE)] Beast, can you recommend some movies \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming the model and tokenizer are already loaded into memory\n",
    "# Remove loading code and pass the model and tokenizer as parameters\n",
    "\n",
    "\n",
    "predict_entities(\"Who is the director of star wars jedi - 4?\", model, tokenizer)\n",
    "predict_entities(\"Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies\", model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atai-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
